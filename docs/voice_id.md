## 相关工作
### 声纹识别
#### 传统方法
声纹识别技术最早可以追溯到20世纪60年代。在传统技术时代，研究人员依赖于声学特征如线性预测倒谱系数（LPC）、感知线性预测系数（PLP）和梅尔频率倒谱系数（MFCC）来提取声音特征。这些特征通过动态时间规整（DTW）和矢量量化（VQ）等模板匹配方法进行分类​。

2000年后，基于高斯混合模型（GMM）和通用背景模型（UBM）的技术成为主流。这种方法通过统计特征建模实现了更高的识别精度，随后引入的联合因子分析（JFA）和i-vector技术进一步改进了模型的鲁棒性和效率​。
#### 深度学习
近年来，深度学习在声纹识别中的应用为该领域带来了质的飞跃。基于深度学习的声纹识别方法突破了传统技术的局限性，特别是在高噪声环境和域不匹配情况下表现出色。深度学习的核心优势在于其强大的特征表达能力，可以从语音信号中提取高度抽象的嵌入特征。

深度学习模型（如DNN、CNN和RNN）能够从时间域或频谱域的输入中提取更具辨识力的特征，如d-vector和x-vector。特别是x-vector方法通过引入统计池化层，从帧级特征提取段级嵌入，在多语种、短语音测试中取得了突破性成果。端到端的声纹识别方法直接从语音波形中学习特征并进行分类。这种方法避免了传统阶段式处理的复杂性，利用联合优化的方式提高了整体性能。
#### ECAPA-TDNN
近年来，随着深度学习技术的快速发展，基于神经网络的声纹识别模型不断涌现，为该领域带来了显著进步。ECAPA-TDNN（Emphasized Channel Attention, Propagation and Aggregation in TDNN）是对传统x-vector架构的深度改进，其设计借鉴了计算机视觉领域的创新方法，如残差网络（ResNet）和通道注意力模块（Squeeze-and-Excitation），显著提升了特征提取和嵌入建模的能力。

ECAPA-TDNN通过多种关键改进增强了模型对语音信号的捕捉能力。首先，模型在特征池化阶段引入了通道和上下文依赖的统计池化机制。这种方法通过自注意力机制动态分配不同时间帧和通道的重要性，同时结合全局上下文信息，更准确地提取与说话人特性相关的特征。其次，模型采用了一维Squeeze-and-Excitation模块和Res2Net结构，在扩大时间上下文范围的同时，有效捕捉了全局通道依赖关系，从而增强了特征表达的鲁棒性。此外，与传统架构只利用深层特征不同，ECAPA-TDNN将所有网络层的输出进行聚合，结合浅层和深层信息，使得嵌入特征更具稳健性。模型还通过改进的残差连接方式整合浅层信息，有效限制了参数量的增加，同时提升了模型性能。

实验结果表明，ECAPA-TDNN在多个主流测试集上的表现优于传统x-vector和ResNet系统。在VoxCeleb1和VoxSRC 2019数据集上，该模型的等错误率（EER）和最小检测成本函数（MinDCF）均实现了显著降低，同时模型参数量却更为精简。尤其是在处理短语音和跨域声纹识别任务时，ECAPA-TDNN表现出极强的适应能力。

---
## 关键技术
### 音频相关模块
本项目涉及音频-特征-文字处理的任务有：本地视频检测模式下，将视频音频利用**语音活动检测**按一轮问答（一次点到-答到）为单位进行切分，然后分离出答到部分音频，随后**提取声纹特征**；实时检测模式下，利用**文本转语音**按照名单产生点名音频，待点名音频播放结束后，通过**语音活动检测**持续监测是否有人答到，当检测到有人答到或是超过最长等待时间，对本轮答到音频进行**声纹特征提取**。
#### 语音活动检测 (VAD)
本项目采用了 [Silero VAD](https://github.com/snakers4/silero-vad) 模型进行语音活动检测。Silero VAD 是一个预训练的企业级语音活动检测器，能够高效、准确地识别音频中的语音片段。其主要特点包括：

- **高精度**：在语音检测任务中表现出色，能够有效区分语音与非语音部分。
- **低延迟**：处理单个音频块（30 毫秒以上）在单个 CPU 线程上耗时不到 1 毫秒，支持实时应用需求。
- **轻量级**：模型大小约为两兆字节，便于在资源受限的环境中部署。
- **通用性**：支持 8000 Hz 和 16000 Hz 采样率，适用于不同领域和背景噪声的音频。

在本地视频检测模式下，Silero VAD 被用于将视频音频按一轮问答（一次点到-答到）为单位进行切分，分离出答到部分音频。Silero VAD 的高精度和低延迟为系统的实时性能提供了重要保障。在实时检测模式下，我们采用传统的阈值方法，先得到音频的包络，如果其绝对值超过了某一阈值，就判定为已经答到。
#### 声纹特征提取（Speaker Embedding）
本项目采用 [ECAPA-TDNN](https://github.com/LKLQQ/ecapa_tdnn) 模型进行声纹特征提取。ECAPA-TDNN（Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network）是一种先进的说话人识别模型，能够从音频数据中提取高维嵌入向量，用于识别和验证答到者的身份。

![ECAPA-TDNN](https://ar5iv.labs.arxiv.org/html/2005.07143/assets/images/full_ecapa.png)

ECAPA-TDNN 模型的主要特点包括：
- **增强的通道注意力机制**：通过引入 SE-block（Squeeze-and-Excitation Block），增强了模型对不同通道特征的关注能力，提高了特征提取的精度。
- **残差连接和多尺度特征聚合**：采用 Res2Block 结构，实现多尺度特征的聚合，捕获更多的语音特征信息。
- **注意力统计池化**：使用 Attentive Statistical Pooling 方法，对特征进行加权聚合，提升了模型对变长语音输入的处理能力。

在本项目中，ECAPA-TDNN 模型被用于从答到音频中提取说话人嵌入向量，这些嵌入向量作为唯一的语音特征标识，用于后续的身份验证。该模型具有高度的准确性和良好的抗噪能力，能够应对复杂的声学环境，确保系统在各种场景下的可靠性。
#### 文本转语音（TTS）
本项目采用 [Edge TTS](https://github.com/rany2/edge-tts) 技术，将文本转换为自然流畅的语音。Edge TTS 是一个开源的 Python 模块，利用 Microsoft Edge 的在线文本转语音服务，无需安装 Microsoft Edge 浏览器或 Windows 系统，也不需要 API 密钥。在本项目中，Edge TTS 被用于生成点名音频。系统根据名单中的姓名，调用 Edge TTS 生成相应的语音片段，并在点名过程中播放。系统支持中文音源（本项目默认音源为 `zh-CN-XiaoxiaoNeural`）和其他语言的多种语音源，生成的语音自然且流畅。

---
## 遇到的问题
### 音频相关部分
#### 视频的答到音频提取
视频的答到音频提取是从多轮对话的连续语音中提取关键词所在音频片段的任务，最靠近关键词唤醒（KWS）任务。但由于数据和精力的限制，多样、大量的答“到”的音频样本难以采集，所以我们采取其他方法实现。由于视频同时存在点到的音频和答到的音频，最开始我们列出的尝试途径有：

1. 利用数据增强训练KWS模型，直接用KWS模型识别“到”所在片段；
2. 用VAD切分音频为每轮问答，或是单独的点到答到，然后使用KWS模型识别“到”所在片段；
3. 用VAD切分音频为每轮问答，然后使用说话人分离模型分离出点到和答到的片段；
4. 用ASR模型识别“到”字，然后获取“到”的时间戳。

由于数据的限制，我们只有小组五个人的答到音频，难以训练一个具有泛化性的KWS模型；并且即使使用现有的TTS模型生成不同的“到”字音频，答到和“到”音频特点实际上和正常说话中的“到”并不相同，所以我们先搁置了途径1和2。而使用途径3和4可以利用他人训练好的模型，这值得我们尝试。

对于途径3，VAD模型在阈值较高时可以分离出单独的点到答到，但许多答到的音频都没有被识别出；调低阈值后，可以稳定地将音频切分成每段问答。然后我们尝试了[SepFormer](https://huggingface.co/speechbrain/sepformer-wsj02mix)说话人分离模型，但是他的效果并不好，可能是因为一声“到”太短了以至于难以被识别成说话人单独的一句话。所以这条途径也不太可行。

对于途径4，我们使用了[Whisper](https://huggingface.co/openai/whisper-large-v3)，然而可能还是由于同样的原因——一声“到”太短了以至于难以被识别成单独的一句话，单独的“到”甚至不会被ASR识别出来，也就无法获取时间戳了。

然后我们画出了途径3中VAD切分出的每轮问答的波形曲线，发现“到”作为一个激励总是出现在问答的后半段，而点名的音频在中点就已经趋于0（虽然后面看来这一点的显然的），所以我们得到了一个可行的途径：用VAD切分音频为每轮问答，然后直接取后一半的音频。这当然很可能不如训练一个KWS模型的精度，但是我们的数据和精力有限就只能这样了。
#### 实时的答到音频检测
实时的答到音频检测任务实际上比视频的答到音频提取更加简单，因为点到的音频是由程序主动播放的，我们知道点到音频的时间戳，只需要检测是否有人答到，这就是很典型的VAD任务。所以我们当然最开始用VAD模型进行检测，我们储存每一轮的音频，直到VAD给出这一轮存在至少一段活动音频。但还是遇到了和之前一样的问题——“到”太短了以至于不能准确地激活VAD模型。最终我们选择了传统的方法，先得到音频的包络，如果其绝对值超过了某一阈值，就判定为已经答到。